# 14MBID-Trabajo de Fin de Master

Explicaci√≥n de los elementos almacenados en el repositorio:
articulos_x_procesar: contiene 1025 art√≠culos producto de la extracci√≥n con el proceso de web scraping (webScraping.py).
datos_base: contiene un archivo CSV usado en el Procesamiento de Lenguaje Natural (estructurar_data.py) para determinar si las localizaciones detectadas con Spacy son un municipio o departamento.
Georreferenciaci√≥n: es un reporte en Power BI usando los datos estandarizados de los art√≠culos (noticias_estandarizadas.json).
chromedriver.exe: es el controlador de versi√≥n 129.0.6668.58 usado en el proceso de web scraping (webScraping.py) con Selenium para la versi√≥n de Google Chrome 128.0.6668.59.
estadisticas.ipynb: cuaderno en el que se realiz√≥ el an√°lisis exploratorio de la informaci√≥n extra√≠da de los art√≠culos (noticias_estandarizadas.json).
estructurar_data.py: c√≥digo Python desarrollado para hacer extracci√≥n de informaci√≥n de los art√≠culos (articulos_x_procesar) a trav√©s de Procesamiento de Lenguaje Natural.
noticias_estandarizadas.json: archivo en formato JSON con los datos extra√≠dos producto del Procesamiento de Lenguaje Natural (estructurar_data.py).
webScraping.py: c√≥digo Python para hacer la extracci√≥n de los art√≠culos de la biblioteca web Internet Archive (Wayback Machine).

# Versi√≥n 2

Proyecto de miner√≠a de textos, centrado en el an√°lisis de art√≠culos sobre violencia machista del diario El Pa√≠s (Espa√±a). Esta nueva versi√≥n est√° dise√±ada para ser modular, reutilizable y f√°cilmente adaptable a otras fuentes de noticias o pa√≠ses.

## üìÇ Estructura del Proyecto

- **text_mining_pipeline.py** ‚Äì Clase que gestiona la extracci√≥n de snapshots, filtrado de URLs y scraping de art√≠culos.

- **test_text_mining.py** ‚Äì Script de prueba para procesar datos de forma controlada por rango de fechas.

- **procesamiento_utils.py** ‚Äì Utilidades compartidas para parseo de fechas, detecci√≥n de ubicaciones y pa√≠ses, y coincidencias por palabras clave.

- **estructurar_data.py** ‚Äì Script completo para procesar los art√≠culos descargados, detectar fechas, delitos, ubicaciones y pa√≠ses, y generar la salida en JSON.

## ‚öôÔ∏è Funcionalidades Principales

1. **Extracci√≥n de Snapshots y URLs** (`text_mining_pipeline.py`)

   - API de Wayback Machine
   - Filtro por palabras clave (`Terminos.csv`)
   - Logs de URLs v√°lidas y fallidas

2. **Scraping de art√≠culos** (`text_mining_pipeline.py`)

   - Selenium en modo `headless`
   - Extracci√≥n de: t√≠tulo, subt√≠tulos, etiquetas, localizaci√≥n y cuerpo
   - Manejo robusto de errores

3. **Procesamiento NLP y estructuraci√≥n** (`main.py`, `estructurar_data.py`)
   - Identificaci√≥n de entidades con SpaCy (`LOC`, `PER`, `GPE`)
   - Detecci√≥n de delitos con regex y normalizaci√≥n
   - Clasificaci√≥n de ubicaciones: municipio, comunidad, provincia
   - Detecci√≥n del pa√≠s desde texto o URL
   - Salida JSON estandarizada

## üß™ Pruebas

Usa test_text_mining.py para probar la tuber√≠a con un solo d√≠a o rango reducido.

## üóÉÔ∏è Dependencias

requests, lxml, pandas, selenium, spacy, pycountry, tldextract
Modelo de SpaCy: es_core_news_lg

> [!IMPORTANT]
> Uno de los primeros problemas detectados fue el uso de ChromeDriver, ya que este var√≠a seg√∫n la versi√≥n del Chrome instalada. Esto se resolvi√≥ consultando la p√°gina oficial de descargas:
>
> üëâ https://developer.chrome.com/docs/chromedriver/downloads?hl=es-419
>
> Desde ah√≠ se puede acceder a los endpoints JSON y ubicar la versi√≥n espec√≠fica requerida seg√∫n el Chrome instalado.
